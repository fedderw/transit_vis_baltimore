{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization/Analysis of Local GTFS-RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from dateutil import tz\n",
    "\n",
    "\n",
    "import branca.colormap as cm\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import seaborn as sns\n",
    "\n",
    "TZ = pytz.timezone('America/Los_Angeles')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corridor_segments = {\n",
    "    0: {\n",
    "        2749,\n",
    "        2751,\n",
    "        2752,\n",
    "        2753,\n",
    "        2754,\n",
    "        2755,\n",
    "        2756,\n",
    "        2757\n",
    "    },\n",
    "    1: {\n",
    "        12200,\n",
    "        12201,\n",
    "        12202,\n",
    "        12203,\n",
    "        12204\n",
    "    },\n",
    "    2: {\n",
    "        13532,\n",
    "        13533,\n",
    "        13534,\n",
    "        13535,\n",
    "        13536\n",
    "    },\n",
    "    3: {\n",
    "        14585,\n",
    "        14586,\n",
    "        14587,\n",
    "        14589,\n",
    "        14590,\n",
    "        14591,\n",
    "        14243\n",
    "    },\n",
    "    4: {\n",
    "        8510,\n",
    "        8511,\n",
    "        8512,\n",
    "        8513,\n",
    "        8514\n",
    "    },\n",
    "    5: {\n",
    "        13221,\n",
    "        13220,\n",
    "        13219,\n",
    "        13218,\n",
    "        13217\n",
    "    },\n",
    "    6: {\n",
    "        18045,\n",
    "        18046,\n",
    "        18047,\n",
    "        18048,\n",
    "        18049\n",
    "    },\n",
    "    7: {\n",
    "        19235,\n",
    "        19234,\n",
    "        11333,\n",
    "        11331,\n",
    "        11330,\n",
    "        11329,\n",
    "        19212\n",
    "    },\n",
    "    8: {\n",
    "        12369,\n",
    "        12366,\n",
    "        12365,\n",
    "        12364,\n",
    "        12363\n",
    "    }\n",
    "}\n",
    "corridor_names = ['2nd Ave; Pike to James',\n",
    "                 'Pike; 3rd to 9th',\n",
    "                 'Westlake; Denny to Mercer',\n",
    "                 'E Olive/E John; Denny to 10th',\n",
    "                 '9th Ave; Alder to Columbia',\n",
    "                 'University Way; Campus Pkwy to 45th',\n",
    "                  'Pacific; 15th to Montlake',\n",
    "                  'NW Market/Leary; 24th to 15th',\n",
    "                  'Rainier; S Bayview to MLK',\n",
    "                  '108th Ave; 4th to 12th']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Local Summarize_RDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-09824a60c791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Some interferences are matched to GTFS data that was assigned to segments outside of the corridor; these are left empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# This only affects traversal times. The tt cannot be calculated for tracked locations outside of a corridor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corridor_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tripid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corridor_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tripid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mrindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         result_data = concatenate_block_managers(\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             b = make_block(\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0m_concatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     to_concat = [\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     to_concat = [\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     ]\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m     )\n\u001b[0;32m-> 1713\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Location for the local data\n",
    "path = './transit_vis/data/to_upload'\n",
    "\n",
    "# Join all the data which is saved in different csv for each day\n",
    "# files = os.listdir(path)\n",
    "# frames = [pd.read_csv(f\"{path}/{filename}\", dtype={'trip_short_name': str, 'route_short_name': str}) for filename in files[0:30]]\n",
    "# all_data = pd.concat(frames)\n",
    "# all_data.to_csv(f\"{path}/../all_data.csv\")\n",
    "# all_data.head()\n",
    "\n",
    "# If already processed, load the data here and comment out above\n",
    "all_data = pd.read_csv(f\"{path}/../all_data.csv\", dtype={'trip_short_name': str, 'route_short_name': str})\n",
    "\n",
    "# Add date to the data\n",
    "all_data['day'] = pd.to_datetime(all_data['locationtime'], unit='s')\n",
    "all_data['day'] = all_data['day'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Add corridor id to the data\n",
    "def get_dict_key(value, corridor_dict):\n",
    "    for corridor in corridor_dict:\n",
    "        if value in corridor_dict[corridor]:\n",
    "            return corridor\n",
    "    return -1\n",
    "all_data['corridor_id'] = all_data['seg_compkey'].apply(get_dict_key, args=(corridor_segments,))\n",
    "\n",
    "# REMOVE ALL DATA NOT IN CORRIDORS\n",
    "# all_data = all_data[all_data['corridor_id']>=0]\n",
    "\n",
    "# Add travel time to the data\n",
    "# Use difference in the first/last time each unique trip is recorded on a corridor's segments for each day\n",
    "tt_data = all_data[all_data['corridor_id']>=0]\n",
    "tt_data = tt_data.groupby(['corridor_id','day','tripid']).agg(['max','min'])[['locationtime']]\n",
    "tt_data.reset_index(inplace=True)\n",
    "tt_data['tt'] = tt_data[('locationtime','max')] - tt_data[('locationtime','min')]\n",
    "tt_data.columns = tt_data.columns.droplevel(1)\n",
    "\n",
    "# Some interferences are matched to GTFS data that was assigned to segments outside of the corridor; these are left empty\n",
    "# This only affects traversal times. The tt cannot be calculated for tracked locations outside of a corridor\n",
    "all_data = pd.merge(all_data, tt_data[['corridor_id','day','tripid','tt']], on=['corridor_id','day','tripid'], how='left')\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distributions of Metrics for Buses with/without Interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corridor_id</th>\n",
       "      <th>corridor_name</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>int_location_tyoe</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>mode</th>\n",
       "      <th>int_start_time</th>\n",
       "      <th>int_stop_time</th>\n",
       "      <th>first_name</th>\n",
       "      <th>difference</th>\n",
       "      <th>report_date</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pike;\\r\\n3rd to 9th</td>\n",
       "      <td>7151</td>\n",
       "      <td>In a driving lane</td>\n",
       "      <td>47.610556</td>\n",
       "      <td>-122.335928</td>\n",
       "      <td>Construction</td>\n",
       "      <td>2021-03-05 07:19:24</td>\n",
       "      <td>2021-03-05 07:19:26</td>\n",
       "      <td>Travis</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>1614957564</td>\n",
       "      <td>1614957566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pike;\\r\\n3rd to 9th</td>\n",
       "      <td>9647</td>\n",
       "      <td>In a driving lane</td>\n",
       "      <td>47.610657</td>\n",
       "      <td>-122.335785</td>\n",
       "      <td>Construction</td>\n",
       "      <td>2021-03-05 07:36:34</td>\n",
       "      <td>2021-03-05 07:36:36</td>\n",
       "      <td>Travis</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>1614958594</td>\n",
       "      <td>1614958596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Pike;\\r\\n3rd to 9th</td>\n",
       "      <td>4508</td>\n",
       "      <td>In a driving lane</td>\n",
       "      <td>47.610609</td>\n",
       "      <td>-122.336568</td>\n",
       "      <td>Construction</td>\n",
       "      <td>2021-03-05 07:38:58</td>\n",
       "      <td>2021-03-05 07:39:00</td>\n",
       "      <td>Travis</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>1614958738</td>\n",
       "      <td>1614958740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>University Way;\\r\\nCampus to 45th</td>\n",
       "      <td>6895</td>\n",
       "      <td>In a driving lane</td>\n",
       "      <td>47.658530</td>\n",
       "      <td>-122.313270</td>\n",
       "      <td>Service Vehicle</td>\n",
       "      <td>2021-03-08 07:48:09</td>\n",
       "      <td>2021-03-08 07:48:11</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-08</td>\n",
       "      <td>1615218489</td>\n",
       "      <td>1615218491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>University Way;\\r\\nCampus to 45th</td>\n",
       "      <td>8048</td>\n",
       "      <td>In a driving lane</td>\n",
       "      <td>47.658636</td>\n",
       "      <td>-122.313270</td>\n",
       "      <td>Pedestrian</td>\n",
       "      <td>2021-03-08 08:04:34</td>\n",
       "      <td>2021-03-08 08:04:38</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-03-08</td>\n",
       "      <td>1615219474</td>\n",
       "      <td>1615219478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  corridor_id                      corridor_name  vehicle_id  \\\n",
       "0           1            1                Pike;\\r\\n3rd to 9th        7151   \n",
       "1           2            1                Pike;\\r\\n3rd to 9th        9647   \n",
       "2           3            1                Pike;\\r\\n3rd to 9th        4508   \n",
       "3           4            5  University Way;\\r\\nCampus to 45th        6895   \n",
       "4           5            5  University Way;\\r\\nCampus to 45th        8048   \n",
       "\n",
       "   int_location_tyoe        lat         lon             mode  \\\n",
       "0  In a driving lane  47.610556 -122.335928     Construction   \n",
       "1  In a driving lane  47.610657 -122.335785     Construction   \n",
       "2  In a driving lane  47.610609 -122.336568     Construction   \n",
       "3  In a driving lane  47.658530 -122.313270  Service Vehicle   \n",
       "4  In a driving lane  47.658636 -122.313270       Pedestrian   \n",
       "\n",
       "        int_start_time        int_stop_time first_name  difference  \\\n",
       "0  2021-03-05 07:19:24  2021-03-05 07:19:26     Travis           2   \n",
       "1  2021-03-05 07:36:34  2021-03-05 07:36:36     Travis           2   \n",
       "2  2021-03-05 07:38:58  2021-03-05 07:39:00     Travis           2   \n",
       "3  2021-03-08 07:48:09  2021-03-08 07:48:11     Andrew           2   \n",
       "4  2021-03-08 08:04:34  2021-03-08 08:04:38     Andrew           4   \n",
       "\n",
       "  report_date  start_time   stop_time  \n",
       "0  2021-03-05  1614957564  1614957566  \n",
       "1  2021-03-05  1614958594  1614958596  \n",
       "2  2021-03-05  1614958738  1614958740  \n",
       "3  2021-03-08  1615218489  1615218491  \n",
       "4  2021-03-08  1615219474  1615219478  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './transit_vis/data/to_upload'\n",
    "\n",
    "interference_data = pd.read_csv(f\"{path}/../interference_instances_new.csv\")\n",
    "interference_data['start_time'] = pd.to_datetime(interference_data['int_start_time']).dt.tz_localize(tz.tzlocal()).astype(int).astype(str).str.slice(0,10).astype(int)\n",
    "interference_data['stop_time'] = pd.to_datetime(interference_data['int_stop_time']).dt.tz_localize(tz.tzlocal()).astype(int).astype(str).str.slice(0,10).astype(int)\n",
    "interference_data['corridor_id'] = interference_data['corridor_id'] - 1\n",
    "interference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Track down the datapoints in GTFS-RT that correspond to each interference instance\n",
    "idx_interference = []  # The index of the interference in the interference data\n",
    "data_interference = []  # Data for +/- xmins from the interference in GTFS-RT\n",
    "datarow_interference = []  # What row of the +/- xmins GTFS-RT data the interference corresponds to\n",
    "\n",
    "for idx, row in interference_data.iterrows():\n",
    "\n",
    "    # Get the tracked datapoints corresponding to +/- xmins of the data; FROM THE START of the interference\n",
    "    found = False\n",
    "    margin_mins = 2\n",
    "    margin = margin_mins*60  # xmins in seconds\n",
    "    data = all_data[all_data['vehicleid'] == row.vehicle_id]\n",
    "    data = data[(data['locationtime']>=(row.start_time - margin)) & (data['locationtime']<=(row.start_time + margin))]\n",
    "    \n",
    "    # Get the metric across the time period where the interference occurred\n",
    "    if len(data) > 0:\n",
    "        data = data.sort_values(by='locationtime', ascending=True)\n",
    "        for j, time in enumerate(data['locationtime']):\n",
    "            if row.start_time <= time:\n",
    "                idx_interference.append(idx)\n",
    "                datarow_interference.append(j-1)\n",
    "                # Get time variable for each interference that is +/- seconds from 0; where 0 is the time of the interference\n",
    "                data['time_from_interference'] = data['locationtime'] - data['locationtime'].iloc[j-1]\n",
    "                # Get schedule deviation variable for each location with interference at 0\n",
    "                data['deviation_from_interference'] = data['scheduledeviation'] - data['scheduledeviation'].iloc[j-1]\n",
    "                data['interference_id'] = str(idx)\n",
    "                data_interference.append(data)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Interference {idx} vehicle id {row.vehicle_id} Fell outside data\")\n",
    "    else:\n",
    "        print(f\"Interference {idx} vehicle id {row.vehicle_id} No data found\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6,4,figsize=(10,10))\n",
    "fig.tight_layout(h_pad=3)\n",
    "\n",
    "for i, idx in enumerate(idx_interference):\n",
    "    sns.ecdfplot(data=data_interference[i], x='speed_m_s', ax=axes.flat[i]).set(xlim=(-120,60), title=f\"Interference #{idx}: {interference_data['difference'][idx]}: {interference_data['corridor_name'][idx]}\")\n",
    "    axes.flat[i].axvline(data_interference[i]['speed_m_s'].iloc[datarow_interference[i]], color='red')\n",
    "    axes.flat[i].axvline(np.median(data_interference[i]['speed_m_s']), color='blue')\n",
    "\n",
    "fig.savefig(fname=\"interference_speeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(9,4,figsize=(10,10))\n",
    "fig.tight_layout(h_pad=3)\n",
    "\n",
    "for i, idx in enumerate(idx_interference):\n",
    "    sns.ecdfplot(data=data_interference[i], x='deviation_change_s', ax=axes.flat[i]).set(xlim=(-120,60), title=f\"Interference #{idx}: {interference_data['difference'][idx]}: {interference_data['corridor_name'][idx]}\")\n",
    "    axes.flat[i].axvline(data_interference[i]['deviation_change_s'].iloc[datarow_interference[i]], color='red')\n",
    "    axes.flat[i].axvline(np.median(data_interference[i]['deviation_change_s']), color='blue')\n",
    "\n",
    "fig.savefig(fname=\"interference_deviation_changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3,figsize=(20,20))\n",
    "fig.tight_layout(h_pad=3)\n",
    "\n",
    "for i in range(9):  # No Bellevue corridor because using the Seattle Streets dataset to map match coords\n",
    "    full_data = all_data[all_data['corridor_id']==i]\n",
    "    sns.ecdfplot(data=full_data, x='tt', ax=axes.flat[i]).set(xlim=(-1,600), title=f\"{corridor_names[i]}\")\n",
    "    for j in range(len(idx_interference)):\n",
    "        if data_interference[j]['corridor_id'].iloc[datarow_interference[j]]==i:\n",
    "            axes.flat[i].axvline(data_interference[j]['tt'].iloc[datarow_interference[j]], color='red')\n",
    "    axes.flat[i].axvline(np.median(full_data['tt']), color='blue')\n",
    "    \n",
    "fig.savefig(fname=\"corridor_traversal_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for de-meaned values in the timeseries data\n",
    "for i in range(len(data_interference)):\n",
    "    norm = np.mean(data_interference[i]['scheduledeviation'])\n",
    "    data_interference[i]['scheduledeviation_norm'] = data_interference[i]['scheduledeviation'] - norm\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(20,10))\n",
    "\n",
    "# Event study timeseries plot per interference\n",
    "sns.lineplot(data=pd.concat(data_interference),\n",
    "             x=\"time_from_interference\",\n",
    "             y=\"scheduledeviation_norm\",\n",
    "             hue=\"interference_id\",\n",
    "             palette='Set2',\n",
    "             marker='o',\n",
    "             ax=axes).set(xlim=(-120,120), xlabel='Time from Interference (s)', ylabel='Schedule Deviation (s)', title=f\"De-Meaned Schedule Deviation +/-{margin_mins}min from Moment of Interference\")\n",
    "\n",
    "# LOWESS regression for data before interference\n",
    "sns.regplot(data=pd.concat(data_interference)[pd.concat(data_interference)['time_from_interference']<=0],\n",
    "             x=\"time_from_interference\",\n",
    "             y=\"scheduledeviation_norm\",\n",
    "             lowess=True,\n",
    "             color='black').set(xlim=(-110,200), xlabel='Time from Interference (s)', ylabel='Schedule Deviation from Interference (s)')\n",
    "\n",
    "# LOWESS regression for data after interference\n",
    "sns.regplot(data=pd.concat(data_interference)[pd.concat(data_interference)['time_from_interference']>0],\n",
    "             x=\"time_from_interference\",\n",
    "             y=\"scheduledeviation_norm\",\n",
    "             lowess=True,\n",
    "             color='black').set(xlim=(-110,200), xlabel='Time from Interference (s)', ylabel='Schedule Deviation from Interference (s)')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "fig.savefig(fname=\"event_study_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Top 10/Top Random Coaches that Spend Most Time in Corridors, and Avg Unique Corridor Traversals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to period of interest\n",
    "# start_time = 1607072400 # 12/4/20 1AM\n",
    "# end_time = 1608282000 # 12/18/20 1AM\n",
    "start_time = 1607518800 # 12/9/20 5AM\n",
    "end_time = 1607583600 # 12/9/20 11PM\n",
    "all_data = all_data.loc[(all_data['locationtime'] > start_time) & (all_data['locationtime'] < end_time)]\n",
    "\n",
    "# Get list of all segments in our corridors\n",
    "segments_dict = list(corridor_segments.values())\n",
    "all_corridor_segments = []\n",
    "for item in segments_dict:\n",
    "    for seg in item:\n",
    "        all_corridor_segments.append(seg)\n",
    "\n",
    "# Filter to only tracked locations in our corridors\n",
    "data = all_data.loc[all_data['seg_compkey'].isin(all_corridor_segments)]\n",
    "del all_data\n",
    "\n",
    "#data.to_csv(\"for_borna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the times that buses are in our corridors\n",
    "z = []\n",
    "dates = data['locationtime'].values\n",
    "dates\n",
    "for time in dates:\n",
    "    x = int(datetime.fromtimestamp(time).strftime(\"%-M\"))/60\n",
    "    y = int(datetime.fromtimestamp(time).strftime(\"%H\"))\n",
    "    z.append(x+y)\n",
    "plt.hist(z, bins=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average number of traversals per corridor\n",
    "# Want count of unique trip ids for each segment id\n",
    "traversal_data = data.groupby(['seg_compkey']).nunique('tripid').reset_index()\n",
    "for corridor in corridor_segments:\n",
    "    corridor_data = traversal_data.loc[traversal_data['seg_compkey'].isin(list(corridor_segments[corridor]))]\n",
    "    print(corridor_data['lat'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top ten vehicles with most tracked locations\n",
    "vehicle_counts = data.groupby('vehicleid').count().reset_index()[['vehicleid', 'lat']]\n",
    "vehicle_list = vehicle_counts.sort_values('lat', ascending=False)\n",
    "vehicle_list = vehicle_list.iloc[:10,:]\n",
    "vehicle_list = vehicle_list['vehicleid'].values\n",
    "corridor_data_10 = data[data['vehicleid'].isin(vehicle_list)]\n",
    "print(f\"Number of tracks for best 10 routes: {len(corridor_data_10)}\")\n",
    "\n",
    "# Get the data corresponding to ten random vehicles\n",
    "vehicle_list = vehicle_counts.sample(10)\n",
    "vehicle_list = vehicle_list.sort_values('lat', ascending=False)\n",
    "vehicle_list = vehicle_list.iloc[0:1,:]\n",
    "vehicle_list = vehicle_list['vehicleid'].values\n",
    "corridor_data_rand = data[data['vehicleid'].isin(vehicle_list)]\n",
    "print(f\"Number of tracks for best random route: {len(corridor_data_rand)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Densities by Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 iterations for random 10 vehicle ids from past two weeks\n",
    "best_route_name = []\n",
    "best_route_count = []\n",
    "for i in range(0, 1000):\n",
    "    sample = vehicle_counts.sample(10)\n",
    "    sample = sample.sort_values('lat', ascending=False)\n",
    "    sample = sample.iloc[0:1,:]\n",
    "    best_route_name.append(sample['vehicleid'].values[0])\n",
    "    best_route_count.append(sample['lat'].values[0])\n",
    "\n",
    "# The top 20 vehicles\n",
    "top_route_name = []\n",
    "top_route_count = []\n",
    "sample = vehicle_counts.sort_values('lat', ascending=False)\n",
    "sample = sample.iloc[:10,:]\n",
    "top_route_name.append(sample['vehicleid'].values)\n",
    "top_route_count.append(sample['lat'].values)\n",
    "top_route_count = list(top_route_count[0])\n",
    "\n",
    "# All vehicles\n",
    "all_route_count = list(vehicle_counts['lat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of tracks across network\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(best_route_count, bw_adjust=1)\n",
    "sns.kdeplot(top_route_count, bw_adjust=1)\n",
    "sns.kdeplot(all_route_count, bw_adjust=1)\n",
    "fig.legend(labels=['Top Sampled Count','Top Un-Sampled Count','All Counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Spatial Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(corridor_data_rand['route_short_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the map\n",
    "f_map = folium.Map(\n",
    "    location=[47.606209, -122.332069],\n",
    "    zoom_start=11,\n",
    "    prefer_canvas=True)\n",
    "\n",
    "# Create and add layer for all the sampled coordinate data\n",
    "# lats = sampled['lat'].tolist()\n",
    "# lons = sampled['lon'].tolist()\n",
    "# points = zip(lats, lons)\n",
    "# points = list(points)\n",
    "# HeatMap(\n",
    "#     data=points,\n",
    "#     name='Points',\n",
    "#     radius=8,\n",
    "#     min_opacity=0.2\n",
    "# ).add_to(f_map)\n",
    "\n",
    "# Create and add layer for all the sampled coordinate data in our corridors\n",
    "lats = corridor_data_rand['lat'].tolist()\n",
    "lons = corridor_data_rand['lon'].tolist()\n",
    "points = zip(lats, lons)\n",
    "points = list(points)\n",
    "HeatMap(\n",
    "    data=points,\n",
    "    name='Points',\n",
    "    radius=8,\n",
    "    min_opacity=0.2\n",
    ").add_to(f_map)\n",
    "\n",
    "folium.LayerControl().add_to(f_map)\n",
    "\n",
    "# Save map and plot in notebook\n",
    "f_map.save(f\"./local_sampled_map.html\")\n",
    "f_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
